# 说明

研究生期间论文博客。

微信:CcQunnnn

# Transformer & BERT

1. Attention Is All You Need

地址：[Transformer：Attention Is All You Need](https://www.jianshu.com/p/01b8f8a696c6)

2. Bert-Bidirectional Encoder Representations from Transformers

地址：[BERT：深度双向预训练语言模型](https://www.jianshu.com/p/0365a760e2b3)

3. CogLTX-Applying BERT to Long Texts

地址：[CogLTX：应用BERT处理长文本](https://www.jianshu.com/p/4aaafa9c0c83)

4. ConvBERT-Improving BERT with Span-based Dynamic Convolution

地址：[ConvBERT：使用基于区间的动态卷积来提升BERT](https://www.jianshu.com/p/3f424a7f12de)

5. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting

地址：[Informer：用于长序列时间序列预测的新型Transformer](https://www.jianshu.com/p/52c9c6acb706)