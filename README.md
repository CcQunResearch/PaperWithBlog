# 说明

研究生期间论文博客。

简书地址:[https://www.jianshu.com/u/fdb45de7ecac](https://www.jianshu.com/u/fdb45de7ecac)

微信:CcQunnnn

# Transformer & BERT

1. Attention Is All You Need

地址：[Transformer：Attention Is All You Need](https://www.jianshu.com/p/01b8f8a696c6)

2. Bert-Bidirectional Encoder Representations from Transformers

地址：[BERT：深度双向预训练语言模型](https://www.jianshu.com/p/0365a760e2b3)

3. CogLTX-Applying BERT to Long Texts

地址：[CogLTX：应用BERT处理长文本](https://www.jianshu.com/p/4aaafa9c0c83)

4. ConvBERT-Improving BERT with Span-based Dynamic Convolution

地址：[ConvBERT：使用基于区间的动态卷积来提升BERT](https://www.jianshu.com/p/3f424a7f12de)

5. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting

地址：[Informer：用于长序列时间序列预测的新型Transformer](https://www.jianshu.com/p/52c9c6acb706)

6. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators

地址：[ELECTRA：类似GAN的预训练语言模型](https://www.jianshu.com/p/1a0dae978e21)

7. Self-Attention Attribution:Interpreting Information Interactions Inside Transformer

地址：[自注意力归因：解释Transformer内部的信息交互](https://www.jianshu.com/p/c386f9026ff6)

8. ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer

地址：[ConSERT：一个自监督对比学习句子表示迁移框架](https://www.jianshu.com/p/ca294681184a)